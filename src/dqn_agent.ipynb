{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb  \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import random, copy\n",
    "import numpy as np\n",
    "from dialogue_config import rule_requests, agent_actions\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"The DQN agent that interacts with the user.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, constants):\n",
    "        \"\"\"\n",
    "        The constructor of DQNAgent.\n",
    "        The constructor of DQNAgent which saves constants, sets up neural network graphs, etc.\n",
    "        Parameters:\n",
    "            state_size (int): The state representation size or length of numpy array\n",
    "            constants (dict): Loaded constants in dict\n",
    "        \"\"\"\n",
    "\n",
    "        self.C = constants['agent']\n",
    "        self.memory = []\n",
    "        self.memory_index = 0\n",
    "        self.max_memory_size = self.C['max_mem_size']\n",
    "        self.eps = self.C['epsilon_init']\n",
    "        self.vanilla = self.C['vanilla']\n",
    "        self.lr = self.C['learning_rate']\n",
    "        self.gamma = self.C['gamma']\n",
    "        self.batch_size = self.C['batch_size']\n",
    "        self.hidden_size = self.C['dqn_hidden_size']\n",
    "\n",
    "        self.load_weights_file_path = self.C['load_weights_file_path']\n",
    "        self.save_weights_file_path = self.C['save_weights_file_path']\n",
    "\n",
    "        if self.max_memory_size < self.batch_size:\n",
    "            raise ValueError('Max memory size must be at least as great as batch size!')\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.possible_actions = agent_actions\n",
    "        self.num_actions = len(self.possible_actions)\n",
    "\n",
    "        self.rule_request_set = rule_requests\n",
    "\n",
    "        self.beh_model = self._build_model()\n",
    "        self.tar_model = self._build_model()\n",
    "\n",
    "        self._load_weights()\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Builds and returns model/graph of neural network.\"\"\"\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.hidden_size, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(self.num_actions, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.lr))\n",
    "        return model\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the rule-based variables.\"\"\"\n",
    "\n",
    "        self.rule_current_slot_index = 0\n",
    "        self.rule_phase = 'not done'\n",
    "\n",
    "    def get_action(self, state, use_rule=False):\n",
    "        \"\"\"\n",
    "        Returns the action of the agent given a state.\n",
    "        Gets the action of the agent given the current state. Either the rule-based policy or the neural networks are\n",
    "        used to respond.\n",
    "        Parameters:\n",
    "            state (numpy.array): The database with format dict(long: dict)\n",
    "            use_rule (bool): Indicates whether or not to use the rule-based policy, which depends on if this was called\n",
    "                             in warmup or training. Default: False\n",
    "        Returns:\n",
    "            int: The index of the action in the possible actions\n",
    "            dict: The action/response itself\n",
    "        \"\"\"\n",
    "\n",
    "        if self.eps > random.random():\n",
    "            index = random.randint(0, self.num_actions - 1)\n",
    "            action = self._map_index_to_action(index)\n",
    "            return index, action\n",
    "        else:\n",
    "            if use_rule:\n",
    "                return self._rule_action()\n",
    "            else:\n",
    "                return self._dqn_action(state)\n",
    "\n",
    "    def _rule_action(self):\n",
    "        \"\"\"\n",
    "        Returns a rule-based policy action.\n",
    "        Selects the next action of a simple rule-based policy.\n",
    "        Returns:\n",
    "            int: The index of the action in the possible actions\n",
    "            dict: The action/response itself\n",
    "        \"\"\"\n",
    "\n",
    "        if self.rule_current_slot_index < len(self.rule_request_set):\n",
    "            slot = self.rule_request_set[self.rule_current_slot_index]\n",
    "            self.rule_current_slot_index += 1\n",
    "            rule_response = {'intent': 'request', 'inform_slots': {}, 'request_slots': {slot: 'UNK'}}\n",
    "        elif self.rule_phase == 'not done':\n",
    "            rule_response = {'intent': 'match_found', 'inform_slots': {}, 'request_slots': {}}\n",
    "            self.rule_phase = 'done'\n",
    "        elif self.rule_phase == 'done':\n",
    "            rule_response = {'intent': 'done', 'inform_slots': {}, 'request_slots': {}}\n",
    "        else:\n",
    "            raise Exception('Should not have reached this clause')\n",
    "\n",
    "        index = self._map_action_to_index(rule_response)\n",
    "        return index, rule_response\n",
    "\n",
    "    def _map_action_to_index(self, response):\n",
    "        \"\"\"\n",
    "        Maps an action to an index from possible actions.\n",
    "        Parameters:\n",
    "            response (dict)\n",
    "        Returns:\n",
    "            int\n",
    "        \"\"\"\n",
    "\n",
    "        for (i, action) in enumerate(self.possible_actions):\n",
    "            if response == action:\n",
    "                return i\n",
    "        raise ValueError('Response: {} not found in possible actions'.format(response))\n",
    "\n",
    "    def _dqn_action(self, state):\n",
    "        \"\"\"\n",
    "        Returns a behavior model output given a state.\n",
    "        Parameters:\n",
    "            state (numpy.array)\n",
    "        Returns:\n",
    "            int: The index of the action in the possible actions\n",
    "            dict: The action/response itself\n",
    "        \"\"\"\n",
    "\n",
    "        index = np.argmax(self._dqn_predict_one(state))\n",
    "        action = self._map_index_to_action(index)\n",
    "        return index, action\n",
    "\n",
    "    def _map_index_to_action(self, index):\n",
    "        \"\"\"\n",
    "        Maps an index to an action in possible actions.\n",
    "        Parameters:\n",
    "            index (int)\n",
    "        Returns:\n",
    "            dict\n",
    "        \"\"\"\n",
    "\n",
    "        for (i, action) in enumerate(self.possible_actions):\n",
    "            if index == i:\n",
    "                return copy.deepcopy(action)\n",
    "        raise ValueError('Index: {} not in range of possible actions'.format(index))\n",
    "\n",
    "    def _dqn_predict_one(self, state, target=False):\n",
    "        \"\"\"\n",
    "        Returns a model prediction given a state.\n",
    "        Parameters:\n",
    "            state (numpy.array)\n",
    "            target (bool)\n",
    "        Returns:\n",
    "            numpy.array\n",
    "        \"\"\"\n",
    "\n",
    "        return self._dqn_predict(state.reshape(1, self.state_size), target=target).flatten()\n",
    "\n",
    "    def _dqn_predict(self, states, target=False):\n",
    "        \"\"\"\n",
    "        Returns a model prediction given an array of states.\n",
    "        Parameters:\n",
    "            states (numpy.array)\n",
    "            target (bool)\n",
    "        Returns:\n",
    "            numpy.array\n",
    "        \"\"\"\n",
    "\n",
    "        if target:\n",
    "            return self.tar_model.predict(states)\n",
    "        else:\n",
    "            return self.beh_model.predict(states)\n",
    "\n",
    "    def add_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Adds an experience tuple made of the parameters to the memory.\n",
    "        Parameters:\n",
    "            state (numpy.array)\n",
    "            action (int)\n",
    "            reward (int)\n",
    "            next_state (numpy.array)\n",
    "            done (bool)\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.memory) < self.max_memory_size:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.memory_index] = (state, action, reward, next_state, done)\n",
    "        self.memory_index = (self.memory_index + 1) % self.max_memory_size\n",
    "\n",
    "    def empty_memory(self):\n",
    "        \"\"\"Empties the memory and resets the memory index.\"\"\"\n",
    "\n",
    "        self.memory = []\n",
    "        self.memory_index = 0\n",
    "\n",
    "    def is_memory_full(self):\n",
    "        \"\"\"Returns true if the memory is full.\"\"\"\n",
    "\n",
    "        return len(self.memory) == self.max_memory_size\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the agent by improving the behavior model given the memory tuples.\n",
    "        Takes batches of memories from the memory pool and processing them. The processing takes the tuples and stacks\n",
    "        them in the correct format for the neural network and calculates the Bellman equation for Q-Learning.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calc. num of batches to run\n",
    "        num_batches = len(self.memory) // self.batch_size\n",
    "        for b in range(num_batches):\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "            states = np.array([sample[0] for sample in batch])\n",
    "            next_states = np.array([sample[3] for sample in batch])\n",
    "\n",
    "            assert states.shape == (self.batch_size, self.state_size), 'States Shape: {}'.format(states.shape)\n",
    "            assert next_states.shape == states.shape\n",
    "\n",
    "            beh_state_preds = self._dqn_predict(states)  # For leveling error\n",
    "            if not self.vanilla:\n",
    "                beh_next_states_preds = self._dqn_predict(next_states)  # For indexing for DDQN\n",
    "            tar_next_state_preds = self._dqn_predict(next_states, target=True)  # For target value for DQN (& DDQN)\n",
    "\n",
    "            inputs = np.zeros((self.batch_size, self.state_size))\n",
    "            targets = np.zeros((self.batch_size, self.num_actions))\n",
    "\n",
    "            for i, (s, a, r, s_, d) in enumerate(batch):\n",
    "                t = beh_state_preds[i]\n",
    "                if not self.vanilla:\n",
    "                    t[a] = r + self.gamma * tar_next_state_preds[i][np.argmax(beh_next_states_preds[i])] * (not d)\n",
    "                else:\n",
    "                    t[a] = r + self.gamma * np.amax(tar_next_state_preds[i]) * (not d)\n",
    "\n",
    "                inputs[i] = s\n",
    "                targets[i] = t\n",
    "\n",
    "            self.beh_model.fit(inputs, targets, epochs=1, verbose=0)\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\"Copies the behavior model's weights into the target model's weights.\"\"\"\n",
    "\n",
    "        self.tar_model.set_weights(self.beh_model.get_weights())\n",
    "\n",
    "    def save_weights(self):\n",
    "        \"\"\"Saves the weights of both models in two h5 files.\"\"\"\n",
    "\n",
    "        if not self.save_weights_file_path:\n",
    "            return\n",
    "        beh_save_file_path = re.sub(r'\\.h5', r'_beh.h5', self.save_weights_file_path)\n",
    "        self.beh_model.save_weights(beh_save_file_path)\n",
    "        tar_save_file_path = re.sub(r'\\.h5', r'_tar.h5', self.save_weights_file_path)\n",
    "        self.tar_model.save_weights(tar_save_file_path)\n",
    "\n",
    "    def _load_weights(self):\n",
    "        \"\"\"Loads the weights of both models from two h5 files.\"\"\"\n",
    "\n",
    "        if not self.load_weights_file_path:\n",
    "            return\n",
    "        beh_load_file_path = re.sub(r'\\.h5', r'_beh.h5', self.load_weights_file_path)\n",
    "        self.beh_model.load_weights(beh_load_file_path)\n",
    "        tar_load_file_path = re.sub(r'\\.h5', r'_tar.h5', self.load_weights_file_path)\n",
    "        self.tar_model.load_weights(tar_load_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
